{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation training of the three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xhorvat9/miniconda3/envs/LTR_classification/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "sys.path.append(\"../../\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the sequence data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTRs = [rec for rec in SeqIO.parse(\"/home/xhorvat9/LTR_classification_data/Sequence_files/train_LTRs.fasta\", \"fasta\")]\n",
    "nonLTRs = [rec for rec in SeqIO.parse(\"/home/xhorvat9/LTR_classification_data/Sequence_files/non_LTRs_training.fasta\", \"fasta\")]\n",
    "\n",
    "\n",
    "\n",
    "LTR_motifs = pd.read_csv(\"~/LTR_classification_data/TFBS/LTR_train_motifCounts.csv\", sep=\"\\t\").set_index(\"ID\")\n",
    "non_LTR_motifs = pd.read_csv(\"~/LTR_classification_data/TFBS/non_LTR_train_motifCounts.csv\", sep=\"\\t\").set_index(\"ID\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that sequences match motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices for LTR sequences and motifs are identical:  True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices for non-LTR sequences and motifs are identical after subsetting:  True\n"
     ]
    }
   ],
   "source": [
    "# LTR ordering is identical to its motif representation\n",
    "print(\"Indices for LTR sequences and motifs are identical: \", all(LTR_motifs[\"ID\"] == [s.id for s in LTRs]))\n",
    "\n",
    "# subset nonLTRs sequences to match the order of the motifs\n",
    "non_LTR_sequence_df = pd.DataFrame({\"sequence\": [str(rec.seq) for rec in nonLTRs], \"ID\": [rec.id for rec in nonLTRs]})\n",
    "non_LTR_sequence_df.set_index(\"ID\", inplace=True)\n",
    "non_LTR_sequence_df = non_LTR_sequence_df[np.invert(non_LTR_sequence_df.index.duplicated(keep='first'))]\n",
    "print(\"Indices for non-LTR sequences and motifs are identical after subsetting: \", all(non_LTR_sequence_df.index == non_LTR_motifs.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([str(s.seq) for s in LTRs] + non_LTR_sequence_df[\"sequence\"].tolist())\n",
    "y = np.array([1]*len(LTRs) + [0]*len(non_LTR_sequence_df))\n",
    "\n",
    "X_motifs = pd.concat([LTR_motifs, non_LTR_motifs], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the BERT_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xhorvat9/miniconda3/envs/LTR_classification/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "  0%|          | 17/346040 [00:00<34:35, 166.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346040/346040 [38:36<00:00, 149.40it/s] \n"
     ]
    }
   ],
   "source": [
    "from utils.BERT_utils import tok_func\n",
    "import tqdm\n",
    "# Preprocess the data \n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('zhihan1996/DNA_bert_6')\n",
    "\n",
    "# cut up sequences longer than 512\n",
    "#def custom_tok(seq, k=510, tokenizer):\n",
    "X_tokenized = []\n",
    "for seq in tqdm.tqdm(X):\n",
    "    sequence_chunks = [seq[i:i+510] for i in range(0, len(seq), 510)]\n",
    "    seq_tokenized = []\n",
    "    for chunk in sequence_chunks:\n",
    "        tokenized_chunk = tokenizer(tok_func(chunk), padding=True, max_length=512, truncation=True)\n",
    "        seq_tokenized.append(tokenized_chunk)\n",
    "    X_tokenized.append(seq_tokenized)\n",
    "#dataset = Dataset(tokenizer([tok_func(x, int(kmer), STRIDE_SIZE) for x in X], padding=True, truncation=True, max_length=512), y)\n",
    "X_tokenized = np.array(X_tokenized, dtype=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(X_tokenized, open(\"X_tokenized.pkl\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at zhihan1996/DNA_bert_6 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from BERT_model import LTRBERT\n",
    "\n",
    "ltrbert_model = LTRBERT(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.CNN_utils import onehote\n",
    "\n",
    "X_OHE = [onehote(x) for x in X]\n",
    "X_OHE = np.array(X_OHE, dtype=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the the OHE vector as .npy object \n",
    "#X_OH = np.asarray(X_OHE, dtype=\"object\")\n",
    "#np.save(\"X_OHE.npy\", X_OH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the K-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "# TODO use StratifiedKFold instead for an even distribution of classes \n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "kf.get_n_splits(X_OHE)\n",
    "split = kf.split(X_OHE, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "batch_size should be a positive integer value, but got batch_size=[1 1 1 ... 0 0 0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[143], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m split \u001b[38;5;241m=\u001b[39m kf\u001b[38;5;241m.\u001b[39msplit(X_OHE, y)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (train_index, test_index) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(split):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Train the BERT model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# TODO fix the subsetting \u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     BERT_train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tokenized\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     BERT_test_dataset \u001b[38;5;241m=\u001b[39m DataLoader(X_tokenized[test_index], y[test_index])\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# TODO \u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Train the CNN\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/LTR_classification/lib/python3.9/site-packages/torch/utils/data/dataloader.py:357\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    353\u001b[0m             sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m batch_sampler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# auto_collation without custom batch_sampler\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m     batch_sampler \u001b[38;5;241m=\u001b[39m \u001b[43mBatchSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_last \u001b[38;5;241m=\u001b[39m drop_last\n",
      "File \u001b[0;32m~/miniconda3/envs/LTR_classification/lib/python3.9/site-packages/torch/utils/data/sampler.py:268\u001b[0m, in \u001b[0;36mBatchSampler.__init__\u001b[0;34m(self, sampler, batch_size, drop_last)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, sampler: Union[Sampler[\u001b[38;5;28mint\u001b[39m], Iterable[\u001b[38;5;28mint\u001b[39m]], batch_size: \u001b[38;5;28mint\u001b[39m, drop_last: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# Since collections.abc.Iterable does not check for `__getitem__`, which\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m# is one way for an object to be an iterable, we don't do an `isinstance`\u001b[39;00m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# check here.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_size, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_size, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[1;32m    267\u001b[0m             batch_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 268\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size should be a positive integer value, but got batch_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(drop_last, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_last should be a boolean value, but got drop_last=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdrop_last\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: batch_size should be a positive integer value, but got batch_size=[1 1 1 ... 0 0 0]"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "split = kf.split(X_OHE, y)\n",
    "for i, (train_index, test_index) in enumerate(split):\n",
    "    # Train the BERT model\n",
    "    # TODO fix the subsetting \n",
    "    BERT_train_dataset = DataLoader(X_tokenized[train_index], y[train_index])\n",
    "    BERT_test_dataset = DataLoader(X_tokenized[test_index], y[test_index])\n",
    "    # TODO \n",
    "\n",
    "\n",
    "    # Train the CNN\n",
    "    OHE_train_X = X_OHE[train_index]\n",
    "    OHE_test_X = X_OHE[test_index]\n",
    "    # TODO \n",
    "\n",
    "\n",
    "    # Train the GBC\n",
    "    # TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OHE_train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mOHE_train_X\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OHE_train_X' is not defined"
     ]
    }
   ],
   "source": [
    "OHE_train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://saturncloud.io/blog/how-to-use-kfold-cross-validation-with-dataloaders-in-pytorch/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnabert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
