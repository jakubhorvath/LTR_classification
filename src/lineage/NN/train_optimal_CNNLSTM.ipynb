{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions and load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 10:19:54.881821: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-02 10:19:54.881880: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-02 10:19:54.881913: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-02 10:19:54.888493: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "import Bio.SeqIO as SeqIO\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, LSTM, Dropout, Bidirectional, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "\n",
    "def balanced_accuracy(y_true, y_pred):\n",
    "    # Convert tensors to NumPy arrays for processing\n",
    "    y_true = tf.make_ndarray(y_true)\n",
    "    y_pred = tf.make_ndarray(y_pred)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    confusion = tf.math.confusion_matrix(y_true, y_pred, num_classes=2)\n",
    "\n",
    "    # Calculate sensitivity (true positive rate) for each class\n",
    "    tp = confusion[1, 1]\n",
    "    fn = confusion[1, 0]\n",
    "    sensitivity = tp / (tp + fn)\n",
    "\n",
    "    # Calculate the balanced accuracy as the average sensitivity\n",
    "    balanced_acc = sensitivity\n",
    "\n",
    "    return balanced_acc\n",
    "\n",
    "def remove_N(seq):\n",
    "    \"\"\"\n",
    "    Remove Ns from sequence\n",
    "    \"\"\"\n",
    "    return seq.upper().replace(\"N\", \"\")\n",
    "\n",
    "def onehote(seq):\n",
    "    \"\"\"\n",
    "    One Hot encoding function\n",
    "    \"\"\"\n",
    "    seq2=list()\n",
    "    mapping = {\"A\":[1., 0., 0., 0.], \"C\": [0., 1., 0., 0.], \"G\": [0, 0., 1., 0.], \"T\":[0., 0., 0., 1.], \"N\":[0., 0., 0., 0.]}\n",
    "    for i in seq:\n",
    "      seq2.append(mapping[i]  if i in mapping.keys() else [0., 0., 0., 0.]) \n",
    "    return np.array(seq2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=3000\n",
    "MIN_LEN=0\n",
    "n_classes = 15\n",
    "LTRs = [rec for rec in SeqIO.parse(\"/var/tmp/xhorvat9/ltr_bert/FASTA_files/train_LTRs.fasta\", \"fasta\") if len(rec.seq) < MAX_LEN+500 and len(rec.seq) > MIN_LEN]\n",
    "n_sequences = len(LTRs)\n",
    "\n",
    "generated, genomic, markov = int(n_sequences*0.2), int(n_sequences*0.5), int(n_sequences*0.3)\n",
    "\n",
    "d = pd.DataFrame({'sequence':[str(rec.seq) for rec in LTRs], 'label':[rec.description.split(\" \")[4] for rec in LTRs]})\n",
    "\n",
    "d = d[~d['label'].str.contains(\"copia\")]\n",
    "d = d[d[\"label\"].isin(d[\"label\"].value_counts()[:n_classes].index.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/134878 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134878/134878 [00:28<00:00, 4701.44it/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Encode labels using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(d['label'])\n",
    "pickle.dump(label_encoder, open(\"/var/tmp/xhorvat9/ltr_bert/NewClassifiers/Lineage/label_encoder.b\", \"wb\"))\n",
    "\n",
    "sequences = [onehote(remove_N(seq)) for seq in tqdm.tqdm(d[\"sequence\"])]\n",
    "#sequences = [onehote(str(rec.seq)) for rec in tqdm.tqdm(LTRs)] + [onehote(str(rec.seq)) for rec in tqdm.tqdm(non_LTRs)]\n",
    "\n",
    "# Split into train and test\n",
    "paddedDNA = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding=\"pre\", maxlen=MAX_LEN)\n",
    "trainX, valX, trainY, valY = train_test_split(paddedDNA, encoded_labels, test_size=0.1, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_weights = class_weight.compute_class_weight('balanced', np.unique(d[\"label\"]), d[\"label\"])\n",
    "label_weights = class_weight.compute_class_weight( class_weight='balanced', classes=np.unique(encoded_labels), y=encoded_labels)\n",
    "weights = {c:w for c, w in zip(np.unique(encoded_labels), label_weights)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run code for short sequence training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1897/1897 [==============================] - 64s 33ms/step - loss: 2.2684 - sparse_categorical_accuracy: 0.2832 - weighted_sparse_categorical_accuracy: 0.2631 - val_loss: 1.8997 - val_sparse_categorical_accuracy: 0.4132 - val_weighted_sparse_categorical_accuracy: 0.4132\n",
      "Epoch 2/15\n",
      "1897/1897 [==============================] - 61s 32ms/step - loss: 1.6792 - sparse_categorical_accuracy: 0.4813 - weighted_sparse_categorical_accuracy: 0.4728 - val_loss: 1.5244 - val_sparse_categorical_accuracy: 0.5380 - val_weighted_sparse_categorical_accuracy: 0.5380\n",
      "Epoch 3/15\n",
      "1897/1897 [==============================] - 61s 32ms/step - loss: 1.3583 - sparse_categorical_accuracy: 0.5764 - weighted_sparse_categorical_accuracy: 0.5847 - val_loss: 1.2869 - val_sparse_categorical_accuracy: 0.6055 - val_weighted_sparse_categorical_accuracy: 0.6055\n",
      "Epoch 4/15\n",
      "1897/1897 [==============================] - 60s 32ms/step - loss: 1.1761 - sparse_categorical_accuracy: 0.6312 - weighted_sparse_categorical_accuracy: 0.6431 - val_loss: 1.1932 - val_sparse_categorical_accuracy: 0.6343 - val_weighted_sparse_categorical_accuracy: 0.6343\n",
      "Epoch 5/15\n",
      "1897/1897 [==============================] - 61s 32ms/step - loss: 1.0561 - sparse_categorical_accuracy: 0.6660 - weighted_sparse_categorical_accuracy: 0.6826 - val_loss: 1.1017 - val_sparse_categorical_accuracy: 0.6632 - val_weighted_sparse_categorical_accuracy: 0.6632\n",
      "Epoch 6/15\n",
      "1897/1897 [==============================] - 60s 32ms/step - loss: 0.9625 - sparse_categorical_accuracy: 0.6924 - weighted_sparse_categorical_accuracy: 0.7125 - val_loss: 1.0680 - val_sparse_categorical_accuracy: 0.6802 - val_weighted_sparse_categorical_accuracy: 0.6802\n",
      "Epoch 7/15\n",
      "1897/1897 [==============================] - 60s 32ms/step - loss: 0.8901 - sparse_categorical_accuracy: 0.7128 - weighted_sparse_categorical_accuracy: 0.7350 - val_loss: 1.0143 - val_sparse_categorical_accuracy: 0.6974 - val_weighted_sparse_categorical_accuracy: 0.6974\n",
      "Epoch 8/15\n",
      "1897/1897 [==============================] - 61s 32ms/step - loss: 0.8307 - sparse_categorical_accuracy: 0.7284 - weighted_sparse_categorical_accuracy: 0.7541 - val_loss: 0.9415 - val_sparse_categorical_accuracy: 0.7179 - val_weighted_sparse_categorical_accuracy: 0.7179\n",
      "Epoch 9/15\n",
      "1897/1897 [==============================] - 61s 32ms/step - loss: 0.7807 - sparse_categorical_accuracy: 0.7402 - weighted_sparse_categorical_accuracy: 0.7668 - val_loss: 0.9658 - val_sparse_categorical_accuracy: 0.7131 - val_weighted_sparse_categorical_accuracy: 0.7131\n",
      "Epoch 10/15\n",
      "1897/1897 [==============================] - 60s 32ms/step - loss: 0.7334 - sparse_categorical_accuracy: 0.7526 - weighted_sparse_categorical_accuracy: 0.7830 - val_loss: 0.9047 - val_sparse_categorical_accuracy: 0.7378 - val_weighted_sparse_categorical_accuracy: 0.7378\n",
      "Epoch 11/15\n",
      "1897/1897 [==============================] - 60s 32ms/step - loss: 0.6933 - sparse_categorical_accuracy: 0.7619 - weighted_sparse_categorical_accuracy: 0.7936 - val_loss: 0.8947 - val_sparse_categorical_accuracy: 0.7390 - val_weighted_sparse_categorical_accuracy: 0.7390\n",
      "Epoch 12/15\n",
      "1897/1897 [==============================] - 60s 32ms/step - loss: 0.6600 - sparse_categorical_accuracy: 0.7704 - weighted_sparse_categorical_accuracy: 0.8033 - val_loss: 0.9231 - val_sparse_categorical_accuracy: 0.7299 - val_weighted_sparse_categorical_accuracy: 0.7299\n",
      "Epoch 13/15\n",
      "1897/1897 [==============================] - 60s 32ms/step - loss: 0.6267 - sparse_categorical_accuracy: 0.7786 - weighted_sparse_categorical_accuracy: 0.8124 - val_loss: 0.9182 - val_sparse_categorical_accuracy: 0.7349 - val_weighted_sparse_categorical_accuracy: 0.7349\n",
      "Epoch 14/15\n",
      "1897/1897 [==============================] - 60s 32ms/step - loss: 0.5991 - sparse_categorical_accuracy: 0.7860 - weighted_sparse_categorical_accuracy: 0.8217 - val_loss: 0.8952 - val_sparse_categorical_accuracy: 0.7394 - val_weighted_sparse_categorical_accuracy: 0.7394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/xhorvat9/tf_CUDA/lib/python3.9/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Conv1D(filters=128, kernel_size=8, padding='same', activation='relu', input_shape=trainX[0].shape))\n",
    "model2.add(Dropout(0.2))  # You can adjust the dropout rate as needed\n",
    "model2.add(MaxPooling1D(pool_size=4))\n",
    "\n",
    "model2.add(LSTM(150))\n",
    "model2.add(Dense(units=n_classes, activation='softmax'))\n",
    "\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'], weighted_metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "#model2.fit(valX, np.array(valY), epochs=3, batch_size=64,verbose = 1,validation_data=(valX, np.array(valY)), callbacks=[WandbCallback()])\n",
    "model2.fit(trainX, trainY, epochs=15, batch_size=64,verbose = 1,validation_data=(valX, valY), callbacks=[EarlyStopping(monitor='val_loss', patience=3)], class_weight=weights)\n",
    "\n",
    "model2.save(\"all_length_cnn_lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31706/31706 [00:34<00:00, 913.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "422/422 [==============================] - 28s 52ms/step - loss: 2.5559 - sparse_categorical_accuracy: 0.1913 - weighted_sparse_categorical_accuracy: 0.1649 - val_loss: 2.4331 - val_sparse_categorical_accuracy: 0.2178 - val_weighted_sparse_categorical_accuracy: 0.2178\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 20s 48ms/step - loss: 2.3123 - sparse_categorical_accuracy: 0.2622 - weighted_sparse_categorical_accuracy: 0.2745 - val_loss: 2.2847 - val_sparse_categorical_accuracy: 0.2483 - val_weighted_sparse_categorical_accuracy: 0.2483\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 20s 48ms/step - loss: 2.0547 - sparse_categorical_accuracy: 0.3290 - weighted_sparse_categorical_accuracy: 0.3569 - val_loss: 2.1439 - val_sparse_categorical_accuracy: 0.3009 - val_weighted_sparse_categorical_accuracy: 0.3009\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 21s 49ms/step - loss: 1.8816 - sparse_categorical_accuracy: 0.3650 - weighted_sparse_categorical_accuracy: 0.4211 - val_loss: 2.0784 - val_sparse_categorical_accuracy: 0.3276 - val_weighted_sparse_categorical_accuracy: 0.3276\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 20s 48ms/step - loss: 1.7587 - sparse_categorical_accuracy: 0.3869 - weighted_sparse_categorical_accuracy: 0.4580 - val_loss: 2.0061 - val_sparse_categorical_accuracy: 0.3656 - val_weighted_sparse_categorical_accuracy: 0.3656\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 21s 50ms/step - loss: 1.6423 - sparse_categorical_accuracy: 0.3959 - weighted_sparse_categorical_accuracy: 0.4899 - val_loss: 1.9176 - val_sparse_categorical_accuracy: 0.3745 - val_weighted_sparse_categorical_accuracy: 0.3745\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 21s 49ms/step - loss: 1.5097 - sparse_categorical_accuracy: 0.4160 - weighted_sparse_categorical_accuracy: 0.5368 - val_loss: 1.9983 - val_sparse_categorical_accuracy: 0.3648 - val_weighted_sparse_categorical_accuracy: 0.3648\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 20s 47ms/step - loss: 1.3965 - sparse_categorical_accuracy: 0.4423 - weighted_sparse_categorical_accuracy: 0.5724 - val_loss: 1.7138 - val_sparse_categorical_accuracy: 0.4415 - val_weighted_sparse_categorical_accuracy: 0.4415\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 21s 49ms/step - loss: 1.2948 - sparse_categorical_accuracy: 0.4611 - weighted_sparse_categorical_accuracy: 0.6049 - val_loss: 1.8375 - val_sparse_categorical_accuracy: 0.4106 - val_weighted_sparse_categorical_accuracy: 0.4106\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 21s 51ms/step - loss: 1.2463 - sparse_categorical_accuracy: 0.4734 - weighted_sparse_categorical_accuracy: 0.6212 - val_loss: 1.6134 - val_sparse_categorical_accuracy: 0.4767 - val_weighted_sparse_categorical_accuracy: 0.4767\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 21s 49ms/step - loss: 1.1341 - sparse_categorical_accuracy: 0.4950 - weighted_sparse_categorical_accuracy: 0.6519 - val_loss: 1.6423 - val_sparse_categorical_accuracy: 0.4710 - val_weighted_sparse_categorical_accuracy: 0.4710\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 20s 48ms/step - loss: 1.0472 - sparse_categorical_accuracy: 0.5126 - weighted_sparse_categorical_accuracy: 0.6821 - val_loss: 1.8681 - val_sparse_categorical_accuracy: 0.4037 - val_weighted_sparse_categorical_accuracy: 0.4037\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 20s 47ms/step - loss: 1.0365 - sparse_categorical_accuracy: 0.5162 - weighted_sparse_categorical_accuracy: 0.6809 - val_loss: 1.6600 - val_sparse_categorical_accuracy: 0.4596 - val_weighted_sparse_categorical_accuracy: 0.4596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/xhorvat9/tf_CUDA/lib/python3.9/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN=3000\n",
    "MIN_LEN=700\n",
    "\n",
    "n_classes = 15\n",
    "LTRs = [rec for rec in SeqIO.parse(\"/var/tmp/xhorvat9/ltr_bert/FASTA_files/train_LTRs.fasta\", \"fasta\") if len(rec.seq) < MAX_LEN and len(rec.seq) > MIN_LEN]\n",
    "n_sequences = len(LTRs)\n",
    "\n",
    "generated, genomic, markov = int(n_sequences*0.2), int(n_sequences*0.5), int(n_sequences*0.3)\n",
    "\n",
    "d = pd.DataFrame({'sequence':[str(rec.seq) for rec in LTRs], 'label':[rec.description.split(\" \")[4] for rec in LTRs]})\n",
    "\n",
    "d = d[~d['label'].str.contains(\"copia\")]\n",
    "d = d[d[\"label\"].isin(d[\"label\"].value_counts()[:n_classes].index.tolist())]\n",
    "\n",
    "# Encode labels using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(d['label'])\n",
    "\n",
    "sequences = [onehote(remove_N(seq)) for seq in tqdm.tqdm(d[\"sequence\"])]\n",
    "#sequences = [onehote(str(rec.seq)) for rec in tqdm.tqdm(LTRs)] + [onehote(str(rec.seq)) for rec in tqdm.tqdm(non_LTRs)]\n",
    "\n",
    "# Split into train and test\n",
    "paddedDNA = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding=\"pre\", maxlen=MAX_LEN)\n",
    "trainX, valX, trainY, valY = train_test_split(paddedDNA, encoded_labels, test_size=0.15, random_state=42)\n",
    "\n",
    "#label_weights = class_weight.compute_class_weight('balanced', np.unique(d[\"label\"]), d[\"label\"])\n",
    "label_weights = class_weight.compute_class_weight( class_weight='balanced', classes=np.unique(encoded_labels), y=encoded_labels)\n",
    "weights = {c:w for c, w in zip(np.unique(encoded_labels), label_weights)}\n",
    "weights\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Conv1D(filters=128, kernel_size=32, padding='same', activation='relu', input_shape=trainX[0].shape))\n",
    "model2.add(Dropout(0.2))  # You can adjust the dropout rate as needed\n",
    "model2.add(MaxPooling1D(pool_size=4))\n",
    "model2.add(LSTM(100))\n",
    "model2.add(Dense(units=n_classes, activation='softmax'))\n",
    "\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'], weighted_metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "#model2.fit(valX, np.array(valY), epochs=3, batch_size=64,verbose = 1,validation_data=(valX, np.array(valY)), callbacks=[WandbCallback()])\n",
    "model2.fit(trainX, trainY, epochs=15, batch_size=64,verbose = 1,validation_data=(valX, valY), callbacks=[EarlyStopping(monitor='val_loss', patience=3)], class_weight=weights)\n",
    "\n",
    "model2.save(\"medium_seq_cnn_lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
