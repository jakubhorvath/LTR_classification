{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1838578/2549366685.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "/data/xhorvat9/LTR_classification_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-08 08:11:58.301547: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-08 08:11:58.314589: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-08 08:11:58.318558: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-08 08:11:58.329629: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-08 08:12:00.008379: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from Bio import SeqIO\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import tqdm\n",
    "import pickle\n",
    "from BERT_model import LTRBERT\n",
    "from CNN_model import Conv1DModel, CNN_dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm \n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, balanced_accuracy_score, matthews_corrcoef\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "test_size = 0.001\n",
    "sys.path.append(\"/data/xhorvat9/LTR_classification/src\")\n",
    "sys.path.append(\"../../\")\n",
    "from utils.BERT_utils import tok_func, Dataset\n",
    "from utils.CNN_utils import onehote\n",
    "import sys\n",
    "\n",
    "def get_embeddings(bert_model, long_sequences):\n",
    "    min_len = 510\n",
    "    window_size = min_len\n",
    "    stride = min_len//3 # ~ 1/3 of window size\n",
    "\n",
    "    outputs = []\n",
    "    sequences = []\n",
    "    counter = 0\n",
    "    for seq in long_sequences:\n",
    "        seq_windows = []\n",
    "        for i in range(0, len(seq), stride):\n",
    "            start = i\n",
    "            end = i + window_size\n",
    "\n",
    "            if end > len(seq):\n",
    "                end = len(seq)\n",
    "            seq_windows.append(seq[start:end])\n",
    "        sequences.append(seq_windows)\n",
    "    \n",
    "    # Get the embeddings for the last layer on the split up sequences\n",
    "    layer_index = -1  # Index of the last layer\n",
    "    token_index = 0   # Index of the token you're interested in\n",
    "    model_embeddings = []\n",
    "    bert_model.eval()\n",
    "    for s in tqdm.tqdm(sequences):\n",
    "        counter += 1\n",
    "    # Tokenize the sequences\n",
    "        tokenized_segment = tokenizer([tok_func(sequence_segment) for sequence_segment in s], padding=True, truncation=True, max_length=min_len+2, return_tensors=\"pt\")\n",
    "        tokenized_segment.to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            # Run model on segments\n",
    "            outputs = bert_model(**tokenized_segment, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states\n",
    "            embeddings = hidden_states[layer_index][:, token_index, :]\n",
    "            embeddings = embeddings.to(\"cpu\")\n",
    "            model_embeddings.append(embeddings)\n",
    "\n",
    "    averaged = []\n",
    "    for emb in tqdm.tqdm(model_embeddings):\n",
    "        averaged.append((sum(emb)/len(emb)).numpy())\n",
    "    averaged = np.array(averaged)\n",
    "    return averaged\n",
    "\n",
    "def pad_sequences(array, max_len):\n",
    "    if len(array) < max_len:\n",
    "        padding = [[0,0,0,0]] * (max_len - len(array))\n",
    "        padded = array + padding\n",
    "        return padded\n",
    "    else:\n",
    "        return array\n",
    "\n",
    "def compute_metrics(p, NO_ARGMAX=False):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    labels = np.argmax(labels, axis=1)\n",
    "\n",
    "    accuracy = balanced_accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred, average='weighted')\n",
    "    precision = precision_score(y_true=labels, y_pred=pred, average='weighted')\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred, average='weighted')\n",
    "    mcc = matthews_corrcoef(y_true=labels, y_pred=pred)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"MCC\": mcc}\n",
    "class CNN_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return pad_sequences(self.data[idx], 4000), self.target[idx]\n",
    "\n",
    "class EmbeddingNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(in_channels=1, out_channels=128, kernel_size=3)\n",
    "        self.pool = torch.nn.MaxPool1d(kernel_size=2)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc1 = torch.nn.Linear(128 * 383, 128)  # Adjusting input size for the Dense layer\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.fc2 = torch.nn.Linear(128, 13)\n",
    "        self.softmax = torch.nn.Softmax(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.nn.ReLU()(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.ReLU()(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "        \n",
    "def select_short_sequences(X, y, max_len=510):\n",
    "    short_sequences = []\n",
    "    short_labels = []\n",
    "    long_sequences = []\n",
    "    long_labels = []\n",
    "    for i, seq in enumerate(X):\n",
    "        if len(seq) <= max_len:\n",
    "            short_sequences.append(seq)\n",
    "            short_labels.append(y[i])\n",
    "        else:\n",
    "            long_sequences.append(seq)\n",
    "            long_labels.append(y[i])\n",
    "    return np.array(short_sequences), np.array(short_labels), np.array(long_sequences), np.array(long_labels)\n",
    "\n",
    "class BERT_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.target[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices for LTR sequences and motifs are identical:  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xhorvat9/LTR_classification_env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "LTRs = [rec for rec in SeqIO.parse(\"/data/xhorvat9/LTR_classification_data/Sequence_files/train_LTRs.fasta\", \"fasta\") if rec.description.split()[3] != \"NAN\"]\n",
    "\n",
    "LTR_motifs = pd.read_csv(\"/data/xhorvat9/LTR_classification_data/TFBS/LTR_train_motifCounts.csv\", sep=\"\\t\").set_index(\"ID\")\n",
    "\n",
    "# LTR ordering is identical to its motif representation\n",
    "LTR_sequence_df = pd.DataFrame({\"sequence\": [str(rec.seq) for rec in LTRs], \"ID\": [rec.id for rec in LTRs], \"label\": [rec.description.split()[4] for rec in LTRs]})\n",
    "LTR_sequence_df.set_index(\"ID\", inplace=True)\n",
    "LTR_sequence_df = LTR_sequence_df.merge(LTR_motifs, on=\"ID\")[[\"sequence\", \"label\"]]\n",
    "LTR_sequence_df = LTR_sequence_df[LTR_sequence_df[\"label\"].isin(list(LTR_sequence_df[\"label\"].value_counts().iloc[:13].index))]\n",
    "X_motifs = LTR_sequence_df.merge(LTR_motifs, on=\"ID\").drop([\"sequence\", \"label\"], axis= 1)\n",
    "print(\"Indices for LTR sequences and motifs are identical: \", all(X_motifs.index == LTR_sequence_df.index))\n",
    "\n",
    "\n",
    "\n",
    "X = np.array(LTR_sequence_df[\"sequence\"].tolist())\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(LTR_sequence_df[\"label\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_indices, _, _, _ = train_test_split([i for i in range(len(X))], y, test_size=test_size, shuffle=True, random_state=42)\n",
    "\n",
    "X = X[X_indices]\n",
    "y = y[X_indices]\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "y_ohe = OneHotEncoder().fit_transform(y.reshape(-1, 1)).toarray()\n",
    "\n",
    "X_motifs = X_motifs.iloc[X_indices, ]\n",
    "\n",
    "# TF-IDF transformation of motifs\n",
    "tfidf = TfidfTransformer()\n",
    "X_motifs = tfidf.fit_transform(X_motifs).toarray()\n",
    "\n",
    "\n",
    "# Preprocess the data \n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('zhihan1996/DNA_bert_6')\n",
    "\n",
    "CNN_input_size = 4000\n",
    "\n",
    "X_OHE = [onehote(x) for x in X]\n",
    "X_OHE = np.array(X_OHE, dtype=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=6, shuffle=True, random_state=42)\n",
    "kf.get_n_splits(X_OHE, y)\n",
    "splits = kf.split(X_OHE, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [(train_index, test_index) for _, (train_index, test_index) in enumerate(splits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save( \"splits.npy\", np.array(arr, dtype=\"object\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "# Define the PyTorch model\n",
    "class Conv1DModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=32, kernel_size=16, padding=8)  # same padding\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=4)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=4, padding=2)  # same padding\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=4)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32 * (4000 // 16), 256)  # Adjust input size accordingly\n",
    "        self.fc2 = nn.Linear(256, 13)\n",
    "        #self.softmax = nn.Softmax(1)\n",
    "\n",
    "    def preprocess(self, x):\n",
    "        \"\"\"\n",
    "            One-hot encoding and removing Ns from the sequences\n",
    "        \"\"\"\n",
    "        x = [remove_N(seq) for seq in x]\n",
    "        x = [onehote(seq) for seq in x]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.preprocess(x)\n",
    "        x = x.to(\"cuda\")\n",
    "\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        #x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5571, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2790.2646, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(104.5699, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(3.6793, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.5308, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.5221, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3829, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.4063, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.4505, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.4742, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3836, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.4019, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3441, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3802, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3521, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3275, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.4162, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3562, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3852, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3983, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3748, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.4502, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3777, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.4027, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3659, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3911, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3908, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3615, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.4002, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3414, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.2821, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3561, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3292, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.2790, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3828, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3005, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3959, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3622, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3477, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3034, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3528, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3406, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3051, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3751, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.2813, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3735, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3557, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3802, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3855, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3597, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.4050, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.4133, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3598, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.2989, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3709, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3462, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3599, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3328, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3081, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3779, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3080, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3494, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3793, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3442, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.4074, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3828, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.4017, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3576, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.4008, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3646, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3603, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3972, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3327, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.2616, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3490, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3262, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.2739, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3768, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3051, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3810, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3472, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3466, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3090, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3544, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3391, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.2911, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3844, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.2726, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3826, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3602, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3750, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3888, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3524, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.4051, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.4105, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3630, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.2963, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3692, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3444, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3639, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3370, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3082, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3757, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3090, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3512, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3739, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3458, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.4057, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3790, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.4000, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3560, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3975, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3638, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3587, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3950, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3335, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.2646, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3456, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3248, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.2729, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3803, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3044, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3831, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3495, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3454, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3072, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3542, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.3400, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(2.2974, device='cuda:0', grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Train the CNN\n",
    "OHE_train_X, OHE_test_X  = X_OHE[train_index], X_OHE[test_index]\n",
    "CNN_model = Conv1DModel()\n",
    "CNN_model = CNN_model.to(\"cuda\")\n",
    "# Define the loss function and optimizer\n",
    "CNN_criterion = torch.nn.CrossEntropyLoss()  # Binary Cross Entropy Loss\n",
    "CNN_optimizer = torch.optim.Adam(CNN_model.parameters(), lr = 0.01 )\n",
    "\n",
    "# Train the CNN\n",
    "OHE_train_X, OHE_test_X  = X_OHE[train_index], X_OHE[test_index]\n",
    "batch_size = 256\n",
    "for epoch in range(3):\n",
    "    for batch_index in range(0, len(OHE_train_X), batch_size):\n",
    "        batch_X = OHE_train_X[batch_index:batch_index+batch_size]\n",
    "        batch_Y = torch.tensor(y_ohe[train_index][batch_index:batch_index+batch_size], dtype=torch.float)\n",
    "\n",
    "        padded_batch_X = torch.tensor(np.array([pad_sequences(x.tolist(), 4000) for x in batch_X]), dtype=torch.float).permute(0, 2, 1).to(\"cuda\")\n",
    "        outputs = CNN_model(padded_batch_X)  # PyTorch expects channels first, so we transpose\n",
    "        #????? should this be before or after running the model \n",
    "        \n",
    "        \n",
    "        loss = CNN_criterion(outputs, batch_Y.cuda())\n",
    "        CNN_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        CNN_optimizer.step()\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xhorvat9/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "padded_test_X = torch.tensor(np.array([pad_sequences(x.tolist(), 4000) for x in OHE_test_X]), dtype=torch.float).permute(0, 2, 1).to(\"cuda\")\n",
    "CNN_predictions = CNN_model(padded_test_X)\n",
    "\n",
    "CNN_metrics = compute_metrics((CNN_predictions.detach().cpu(), y_ohe[test_index]), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at zhihan1996/DNA_bert_6 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/data/xhorvat9/LTR_classification_env/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/xhorvat9/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  6/550 00:02 < 04:54, 1.85 it/s, Epoch 0.05/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 45\u001b[0m\n\u001b[1;32m     24\u001b[0m args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     25\u001b[0m output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLTRBERT_LTR_train\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m push_to_hub\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,)\n\u001b[1;32m     36\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     37\u001b[0m     model\u001b[38;5;241m=\u001b[39mbert_model,\n\u001b[1;32m     38\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)],\n\u001b[1;32m     43\u001b[0m )\n\u001b[0;32m---> 45\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Run model on segments of long sequences\u001b[39;00m\n\u001b[1;32m     49\u001b[0m averaged \u001b[38;5;241m=\u001b[39m get_embeddings(bert_model, BERT_train_X_long)\n",
      "File \u001b[0;32m/data/xhorvat9/LTR_classification_env/lib/python3.9/site-packages/transformers/trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   1928\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 1929\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1930\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1936\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[0;32m/data/xhorvat9/LTR_classification_env/lib/python3.9/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/data/xhorvat9/LTR_classification_env/lib/python3.9/site-packages/transformers/trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/data/xhorvat9/LTR_classification_env/lib/python3.9/site-packages/accelerate/accelerator.py:2196\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2196\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "old_stdout = sys.stdout\n",
    "log_file = open(\"training_out.log\",\"w+\")\n",
    "\n",
    "sys.stdout = log_file\n",
    "\n",
    "#np.save(\"splits.npy\", np.array(splits, dtype=\"object\"))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(splits):\n",
    "\n",
    "    # Train the BERT model\n",
    "\n",
    "    bert_model = transformers.BertForSequenceClassification.from_pretrained('zhihan1996/DNA_bert_6', num_labels=13)\n",
    "\n",
    "    # Tokenize the short sequences\n",
    "    BERT_train_X, train_y = X[train_index], y[train_index]\n",
    "    BERT_train_X_short, train_y_short, BERT_train_X_long, train_y_long = select_short_sequences(BERT_train_X, y_ohe[train_index])\n",
    "    \n",
    "    BERT_test_X_short, test_y_short, BERT_test_X_long, test_y_long = select_short_sequences(X[test_index], y_ohe[test_index])\n",
    "\n",
    "    train_dataset = Dataset(tokenizer([tok_func(x) for x in BERT_train_X_short], padding=True, truncation=True, max_length=512), train_y_short)\n",
    "    val_dataset = Dataset(tokenizer([tok_func(x) for x in BERT_test_X_short], padding=True, truncation=True, max_length=512), test_y_short)\n",
    "\n",
    "    # Train BERT on short sequences \n",
    "    args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    seed=0,\n",
    "    load_best_model_at_end=True,\n",
    "    run_name=\"LTRBERT_LTR_train\",\n",
    "    push_to_hub=True,)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=bert_model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "\n",
    "    trainer.train()        \n",
    "\n",
    "    # Run model on segments of long sequences\n",
    "\n",
    "    averaged = get_embeddings(bert_model, BERT_train_X_long)\n",
    "\n",
    "    emb_model = EmbeddingNet().cuda()\n",
    "    criterion = torch.nn.CrossEntropyLoss()  # Binary Cross Entropy Loss\n",
    "    optimizer = torch.optim.Adam(emb_model.parameters())  # Adam optimizer\n",
    "\n",
    "    batch_size = 32\n",
    "    for epoch in range(3):\n",
    "        for batch_index in range(0, len(averaged), batch_size):\n",
    "            batch_X = torch.tensor(averaged[batch_index:batch_index+batch_size, :], dtype=torch.float).unsqueeze(1).cuda()\n",
    "            batch_Y = torch.tensor(train_y_long[batch_index:batch_index+batch_size], dtype=torch.float)\n",
    "\n",
    "            outputs = emb_model(batch_X)  # PyTorch expects channels first, so we transpose\n",
    "            #????? should this be before or after running the model \n",
    "            optimizer.zero_grad()\n",
    "            #test_y_long[batch_index:batch_index+batch_size].reshape(-1,1)\n",
    "            loss = criterion(outputs.cpu(), batch_Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "    # Train the CNN\n",
    "    OHE_train_X, OHE_test_X  = X_OHE[train_index], X_OHE[test_index]\n",
    "    CNN_model = Conv1DModel()\n",
    "    CNN_model = CNN_model.to(\"cuda\")\n",
    "    # Define the loss function and optimizer\n",
    "    CNN_criterion = torch.nn.CrossEntropyLoss()  # Binary Cross Entropy Loss\n",
    "    CNN_optimizer = torch.optim.Adam(CNN_model.parameters(), lr = 0.01 )\n",
    "\n",
    "    # Train the CNN\n",
    "    OHE_train_X, OHE_test_X  = X_OHE[train_index], X_OHE[test_index]\n",
    "    batch_size = 256\n",
    "    for epoch in range(3):\n",
    "        for batch_index in range(0, len(OHE_train_X), batch_size):\n",
    "            batch_X = OHE_train_X[batch_index:batch_index+batch_size]\n",
    "            batch_Y = torch.tensor(y_ohe[train_index][batch_index:batch_index+batch_size], dtype=torch.float)\n",
    "    \n",
    "            padded_batch_X = torch.tensor(np.array([pad_sequences(x.tolist(), 4000) for x in batch_X]), dtype=torch.float).permute(0, 2, 1).to(\"cuda\")\n",
    "            outputs = CNN_model(padded_batch_X)  # PyTorch expects channels first, so we transpose\n",
    "            #????? should this be before or after running the model \n",
    "            \n",
    "            \n",
    "            loss = CNN_criterion(outputs, batch_Y.cuda())\n",
    "            CNN_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            CNN_optimizer.step()\n",
    "\n",
    "    \n",
    "    #GBC = GradientBoostingClassifier(max_depth=8, min_samples_leaf=50, n_estimators=400)\n",
    "    # Train the GBC\n",
    "    #X_motifs_train, X_motifs_test = X_motifs[train_index], X_motifs[test_index]\n",
    "    #GBC.fit(X_motifs_train, train_y)\n",
    "\n",
    "\n",
    "    # Test the trained classifiers \n",
    "    #raw_pred, _, _ = trainer.predict(Dataset(tokenizer([tok_func(x) for x in BERT_test_X_short], padding=True, truncation=True, max_length=512), test_y_short))\n",
    "    #bert_short_predictions = np.argmax(raw_pred, axis=1)\n",
    "    bert_short_predictions, _, _ = trainer.predict(Dataset(tokenizer([tok_func(x) for x in BERT_test_X_short], padding=True, truncation=True, max_length=512)))\n",
    "    emb_model.eval()\n",
    "    bert_long_predictions = emb_model(torch.tensor(get_embeddings(bert_model, BERT_test_X_long), dtype=torch.float).unsqueeze(1).cuda())\n",
    "\n",
    "    CNN_model.eval()\n",
    "    # TODO might be the cause of memory issues here \n",
    "    padded_test_X = torch.tensor(np.array([pad_sequences(x.tolist(), 4000) for x in OHE_test_X]), dtype=torch.float).permute(0, 2, 1).to(\"cuda\")\n",
    "    CNN_predictions = CNN_model(padded_test_X)\n",
    "    \n",
    "    #GBC_predictions = GBC.predict(X_motifs_test)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    bert_short_metrics = compute_metrics((bert_short_predictions, test_y_short), True)\n",
    "    bert_long_metrics = compute_metrics((bert_long_predictions.detach().cpu(), test_y_long), True)\n",
    "\n",
    "    CNN_metrics = compute_metrics((CNN_predictions.detach().cpu(), y_ohe[test_index]), True)\n",
    "\n",
    "    #GBC_metrics = compute_metrics((GBC_predictions, y[test_index]), True)\n",
    "    print(f\"Split {i}\")\n",
    "    print(f\"BERT short sequence metrics: {bert_short_metrics}\")\n",
    "    print(f\"BERT long sequence metrics: {bert_long_metrics}\")\n",
    "    print(f\"CNN metrics: {CNN_metrics}\")\n",
    "    #print(f\"GBC metrics: {GBC_metrics}\")\n",
    "\n",
    "\n",
    "sys.stdout = old_stdout\n",
    "\n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 796/796 [00:33<00:00, 23.83it/s]\n",
      "100%|██████████| 796/796 [00:00<00:00, 47732.05it/s]\n",
      "/tmp/ipykernel_1174315/52773131.py:124: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0\n",
      "BERT short sequence metrics: {'accuracy': 0.07692307692307693, 'precision': 0.03360239397152224, 'recall': 0.1833095577746077, 'f1': 0.05679391964807014, 'MCC': 0.0}\n",
      "BERT long sequence metrics: {'accuracy': 0.07692307692307693, 'precision': 0.03182369132092624, 'recall': 0.17839195979899497, 'f1': 0.05401206458732897, 'MCC': 0.0}\n",
      "CNN metrics: {'accuracy': 0.07692307692307693, 'precision': 0.03295265528013307, 'recall': 0.18152866242038215, 'f1': 0.05577969680841932, 'MCC': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xhorvat9/LTR_classification/src/lineage/CrossVal_traning/CNN_model.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n",
      "/home/xhorvat9/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/xhorvat9/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/xhorvat9/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "    bert_short_predictions, _, _ = trainer.predict(Dataset(tokenizer([tok_func(x) for x in BERT_test_X_short], padding=True, truncation=True, max_length=512)))\n",
    "    emb_model.eval()\n",
    "    bert_long_predictions = emb_model(torch.tensor(get_embeddings(bert_model, BERT_test_X_long), dtype=torch.float).unsqueeze(1).cuda())\n",
    "\n",
    "    CNN_model.eval()\n",
    "    # TODO might be the cause of memory issues here \n",
    "    padded_test_X = torch.tensor(np.array([pad_sequences(x.tolist(), 4000) for x in OHE_test_X]), dtype=torch.float).permute(0, 2, 1).to(\"cuda\")\n",
    "    CNN_predictions = CNN_model(padded_test_X)\n",
    "    \n",
    "    #GBC_predictions = GBC.predict(X_motifs_test)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    bert_short_metrics = compute_metrics((bert_short_predictions, test_y_short), True)\n",
    "    bert_long_metrics = compute_metrics((bert_long_predictions.detach().cpu(), test_y_long), True)\n",
    "\n",
    "    CNN_metrics = compute_metrics((CNN_predictions.detach().cpu(), y_ohe[test_index]), True)\n",
    "\n",
    "    #GBC_metrics = compute_metrics((GBC_predictions, y[test_index]), True)\n",
    "    print(f\"Split {i}\")\n",
    "    print(f\"BERT short sequence metrics: {bert_short_metrics}\")\n",
    "    print(f\"BERT long sequence metrics: {bert_long_metrics}\")\n",
    "    print(f\"CNN metrics: {CNN_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xhorvat9/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/xhorvat9/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/xhorvat9/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.07692307692307693, 'precision': 0.03360239397152224, 'recall': 0.1833095577746077, 'f1': 0.05679391964807014, 'MCC': 0.0} {'accuracy': 0.07692307692307693, 'precision': 0.03182369132092624, 'recall': 0.17839195979899497, 'f1': 0.05401206458732897, 'MCC': 0.0} {'accuracy': 0.07692307692307693, 'precision': 0.03295265528013307, 'recall': 0.18152866242038215, 'f1': 0.05577969680841932, 'MCC': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(bert_short_metrics, bert_long_metrics, CNN_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
